from langgraph.graph import StateGraph, END
from langchain_core.runnables import Runnable
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json

# ğŸ”¹ Qwen ë¡œë“œ
model_name = "Qwen/Qwen1.5-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map="auto")
model.eval()

# ğŸ”¹ ê°„ë‹¨í•œ íˆ´ ì •ì˜ (ì˜ˆ: ê³„ì‚°ê¸°)
def calculator_tool(input_str):
    try:
        result = eval(input_str)
        return f"ê³„ì‚° ê²°ê³¼ëŠ” {result}ì…ë‹ˆë‹¤."
    except Exception as e:
        return f"ê³„ì‚° ì˜¤ë¥˜: {e}"

# ğŸ”¹ ë¼ìš°í„°: íˆ´ í˜¸ì¶œ ì—¬ë¶€ íŒë‹¨
class ToolRouter(Runnable):
    def invoke(self, state, config=None):
        last_message = state["messages"][-1]["content"]
        if "ê³„ì‚°í•´ì¤˜" in last_message or any(op in last_message for op in ["+", "-", "*", "/"]):
            return {"next": "Tool", "messages": state["messages"]}
        else:
            return {"next": "Qwen", "messages": state["messages"]}

# ğŸ”¹ íˆ´ ì‹¤í–‰ ë…¸ë“œ
class ToolNode(Runnable):
    def invoke(self, state, config=None):
        input_str = state["messages"][-1]["content"]
        tool_result = calculator_tool(input_str)
        return {"messages": state["messages"] + [{"role": "assistant", "content": tool_result}]}

# ğŸ”¹ Qwen ì‘ë‹µ ë…¸ë“œ
class QwenNode(Runnable):
    def invoke(self, state, config=None):
        prompt = state["messages"][-1]["content"]
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                eos_token_id=tokenizer.eos_token_id,
            )
        result = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)
        return {"messages": state["messages"] + [{"role": "assistant", "content": result.strip()}]}

# ğŸ”¹ ìƒíƒœ ë¨¸ì‹  êµ¬ì„±
builder = StateGraph(dict)

builder.add_node("Router", ToolRouter())
builder.add_node("Tool", ToolNode())
builder.add_node("Qwen", QwenNode())

# íë¦„ ì •ì˜
builder.set_entry_point("Router")
builder.add_conditional_edges(
    "Router",
    lambda state: state["next"],
    {
        "Tool": "Tool",
        "Qwen": "Qwen"
    }
)
builder.set_finish_point("Tool")
builder.set_finish_point("Qwen")

graph = builder.compile()

# ğŸ”¹ í…ŒìŠ¤íŠ¸
def run_chat(prompt):
    print(f"User: {prompt}")
    state = {"messages": [{"role": "user", "content": prompt}]}
    result = graph.invoke(state)
    print("Assistant:", result["messages"][-1]["content"])

# âœ… ì˜ˆì œ ì‹¤í–‰
run_chat("2 + 3")
run_chat("what is the capital of France?")
